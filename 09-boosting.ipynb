{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "______                 _   _\n",
    "| ___ \\               | | (_)\n",
    "| |_/ / ___   ___  ___| |_ _ _ __   __ _\n",
    "| ___ \\/ _ \\ / _ \\/ __| __| | '_ \\ / _` |\n",
    "| |_/ / (_) | (_) \\__ \\ |_| | | | | (_| |\n",
    "\\____/ \\___/ \\___/|___/\\__|_|_| |_|\\__, |\n",
    "                                    __/ |\n",
    "                                   |___/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Motivation\n",
    "Boosting is to use when ensembling is not. The goal of boosting is to build on\n",
    "a low-complexity hypothesis space. This is done combining a series of base\n",
    "models (aka. weak learners) from the low-complexity space.\n",
    "\n",
    "# Boosting as fitting residuals\n",
    "Whereas ensembles are built with independent base models, boosting is not.\n",
    "Actually, one way to approach boosting is to see it as adding a new model into\n",
    "a bigger one by fitting the residuals of the big model.\n",
    "\n",
    "More formally, at stage $t$ the following optimization problem is solved:\n",
    "> $$f_{[t]}, w_{[t]} = \\arg\\min_{f, w} \\sum_{i=1}^n \\ell \\left(y_i, \\hat{y}_{[t]}(x_i) + w f(x_i) \\right)$$\n",
    "\n",
    "where $\\{(x_i, y_i)\\}_{i=1}^n$ is the training set, and $\\hat{y}_{[t]}$ is the\n",
    "big model at stage $t$. A new simple model $f_{[t]}$ is added, together with its\n",
    "weight $w_{[t]}$ (sometimes the weight is omitted).\n",
    "\n",
    "Therefore the final model looks like\n",
    ">$$\\hat{y}_{[t]}(x) = \\hat{y}_{[0]}(x) + \\sum_{\\tau=1}^t \\lambda w_{[\\tau]} f_{[\\tau]}(x)$$\n",
    "\n",
    "$\\lambda \\leq 1$ is called the learning rate. Setting $\\lambda < 1$ serves to\n",
    "regularize learning. $\\hat{y}_{[0]}(x)$ is usually either $0$ or the best\n",
    "constant over the training set.\n",
    "\n",
    "\n",
    "# Boosting in practice\n",
    "Boosting as described in the previous section is a generic framework. To deploy\n",
    "it in practice we need to\n",
    "- choose the low-complexity hypothesis space;\n",
    "- define the loss more precisely (and come up with a way to solve the\n",
    "optimization program).\n",
    "\n",
    "Most often, stumps (decision tree of depth 1) are chosen as base learners.\n",
    "The choice of the loss depends on whether it is classification or regression\n",
    "problem.\n",
    "\n",
    "## Adaboost\n",
    "[Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "is a specific instance of the boosting framework with stumps where the loss is\n",
    "the `exponential loss`, a specific loss for classification which leads to a\n",
    "closed-form solution of the optimization problem.\n",
    "\n",
    "> TODO example\n",
    "\n",
    "## Gradient boosting\n",
    "Gradient boosting is a generic method for losses whose gradient can be computed.\n",
    "For regression under the squared error loss, this becomes the\n",
    "`least-square boosting`.\n",
    "\n",
    "> TODO example\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}