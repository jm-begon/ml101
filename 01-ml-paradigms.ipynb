{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "___  ___           _     _              _                       _\n",
    "|  \\/  |          | |   (_)            | |                     (_)\n",
    "| .  . | __ _  ___| |__  _ _ __   ___  | | ___  __ _ _ __ _ __  _ _ __   __ _\n",
    "| |\\/| |/ _` |/ __| '_ \\| | '_ \\ / _ \\ | |/ _ \\/ _` | '__| '_ \\| | '_ \\ / _` |\n",
    "| |  | | (_| | (__| | | | | | | |  __/ | |  __/ (_| | |  | | | | | | | | (_| |\n",
    "\\_|  |_/\\__,_|\\___|_| |_|_|_| |_|\\___| |_|\\___|\\__,_|_|  |_| |_|_|_| |_|\\__, |\n",
    "                                                                         __/ |\n",
    "                                                                        |___/\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Supervised learning\n",
    "In `supervised learning`, we have $x$, the `input`, and $y$, the `output`\n",
    "(or `label`).\n",
    "For instance, $x$ can be some bio-medical information about a patient or a\n",
    "customer. $y$ can respectively be whether the patient is healthy or sick, or\n",
    "the likelihood of the customer shurning.\n",
    "\n",
    "The collection of admissible inputs and outputs are respectively called the\n",
    "`input space` $\\mathbb{X}$ and `output space` $\\mathbb{Y}$.\n",
    "\n",
    "In supervised learning, the goal is to come up with some function\n",
    "$\\hat{y} : \\mathbb{X} \\rightarrow \\mathbb{Y}$ modelling some phenomenon.\n",
    "\n",
    "$\\hat{y} \\in \\mathbb{H}$ is known as the `hypothesis`, the `model`,\n",
    "the `predictor` (classifier or regressor).\n",
    "$\\mathbb{H}$ is called the `hypothesis space` (or `model class).\n",
    "\n",
    "Supervised learning works in two stages:\n",
    "\n",
    "1. **Learning/training**: a *good* model $\\hat{y}_*$ is selected from\n",
    "$\\mathbb{H}$;\n",
    "2. **Prediction/inference**: $\\hat{y}_*$ is used to make prediction on (new)\n",
    "data $x \\in \\mathbb{X}$.\n",
    "\n",
    "> ###### Distribution (technical note)\n",
    "> Not all (x, y) pair are equally likely (or even possible). This is captured\n",
    "> through the notion of a probabilistic distribution $\\mathcal{D}$.\n",
    "> $(x, y) \\sim  \\mathcal{D}$ indicates that the labeled pair $(x, y)$ has been\n",
    "> drawn from $\\mathcal{D}$.\n",
    "\n",
    "\n",
    "## Data\n",
    "To select the best hypothesis, we have a `learning set` of labeled pairs\n",
    "$LS = \\{(x_i, y_i) \\}_{i=1}^n$.\n",
    "\n",
    "### Inputs\n",
    "Typically, we will assume that $\\mathbb{X} \\subseteq \\mathbb{R}^p$. Therefore,\n",
    "it is customary to group the inputs into a `learning matrix` $X$ such that the\n",
    "element at the $i$th row and $j$th column is the $j$th component of the vector\n",
    "corresponding to instance $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Transforming images to a learning matrix\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "print(X.shape)\n",
    "X  # each correspond to a sample, each column to a variable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dimensions of the vectors are called (input) `variable`, or `features`.\n",
    "\n",
    "Coming up with the appropriate features is not trivial. This is either done\n",
    "manually (`feature engineering`) or as part of learning the hypothesis\n",
    "(`representation learning`).\n",
    "\n",
    "\n",
    "### Classification and regression\n",
    "The nature of $\\mathbb{Y}$ dictates the type of problems. When the output\n",
    "variable is discrete (eg. healthy/sick), the problem is known as\n",
    "`classification`. The output can be referred to as the `class`. A hypothesis\n",
    "can further be called a `classifier` in this setting.\n",
    "\n",
    "When the output is continuous (eg. the number of cases, the gross production),\n",
    "the problem is a `regression`. A hypothesis can be referred to as a `regressor`\n",
    "in this setting.\n",
    "\n",
    "> ###### Class probabilities and encoding (technical note)\n",
    "> The discrete nature of the output in classification is often quite limiting.\n",
    "> A common work-around is for the classifier to output a vector $\\hat{p} of\n",
    "> size $K$ (where $K$ is the number of classes)\n",
    "> indicating the probability (according to the model) of belonging to each\n",
    "> class.\n",
    ">\n",
    "> The true output must sometime also match this representation, in which case\n",
    "> classes are encoded in a one-hot vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([0, 1, 1, 2, 0, 1])  # y[i] is the class of the ith instance\n",
    "OneHotEncoder().fit_transform(y.reshape((-1, 1))).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss and error\n",
    "\n",
    "We, of course, want a hypothesis that models well the phenomenon. For that, we\n",
    "have the notion of `loss function`.\n",
    "\n",
    "### Loss\n",
    "A loss function $\\ell$ measures how far away a prediction $\\hat{y}$ falls\n",
    "from the truth $y$.\n",
    "\n",
    "In regression, the most common loss function is the `squared error`:\n",
    "> $$\\ell(y, \\hat{y}) = (y - \\hat{y})^2$$\n",
    "\n",
    "In classification, we usually use the `zero-one` loss, which indicates\n",
    "whether the model is making a mistake:\n",
    "> $$\\mathbb{I}(y \\neq \\hat{y})$$\n",
    "\n",
    "When working with class probabilities (where $K$ is the number of classes),\n",
    "a common choice of loss is the cross-entropy:\n",
    "> $$\\sum_{j=1}^K y^{(j)} \\log \\hat{p}^{(j)}(x)$$\n",
    "\n",
    "> ###### Note on binary classification metrics\n",
    "> There are also many specific metrics for binary classification (specificity,\n",
    "> sensitivity, recall, auroc, aupr, FPR, F1-score, etc). This is\n",
    "> motivated by the fact that not all errors should have the same weight. For\n",
    "> instance, it might be better to wrongly diagnose a cancer (an error which can\n",
    "> be caught later on) than to miss one.\n",
    "\n",
    "###  Error (risk)\n",
    "We usually want to know how the model performs in general. This is captured by\n",
    "the notion of `error` or `risk`, which consists in taking the\n",
    "average/expectation of the loss.\n",
    "\n",
    "The error based on the squared error is the `mean squared error` (MSE).\n",
    "Sometimes, it is preferable to take the root of the MSE, which is known as the\n",
    "`root mean squared error` (RMSE).\n",
    "\n",
    "The error based on the zero-one loss is known as the\n",
    "`misclassification rate`. It is the average (or expected) number of mistakes\n",
    "the model makes. Alternatively, the average number of correct predictions\n",
    "the model makes is known as the `accuracy`.\n",
    "\n",
    "\n",
    "> ###### Empirical vs. expected risk (technical note)\n",
    "> The goal of supervised learning is\n",
    ">> $$\\hat{y}_* = \\arg\\min_{\\hat{y} \\in \\mathbb{H}} \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\{\\ell(y, \\hat{y}(x)) \\}$$\n",
    "> In words, we want to minimize the expected risk\n",
    ">\n",
    "> In practice, we do not have to the whole distribution. Rather we can estimate\n",
    "> the risk given some set $S = \\{(x_i, y_i)\\}_{i=1}^n$:\n",
    ">> $$\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, \\hat{y}(x_i))\n",
    ">\n",
    "> For the empirical error to be a reliable estimate of the expected risk, some\n",
    "> precaution must be observed (see `overfitting` and `data shift`).\n",
    "\n",
    "# Related paradigms\n",
    "In the following paradigms, the goal remains to learn a good predictor:\n",
    "\n",
    "- *semi-supervised learning*: we have access to unlabeled data\n",
    "(only $x$ samples) in addition to the traditional learning set;\n",
    "- *few-shot learning*: we only have a (too) small learning set;\n",
    "- *zero-shot learning*: we have no learning set;\n",
    "- *active learning*: the goal is o decide which inputs should be labeled to\n",
    "improve the performance the best;\n",
    "- *transfer learning*/*domain adaptation*: leverage knowledge learned on a\n",
    "source task to help\n",
    "in a target task;\n",
    "- *transductive learning*: making predictions without explicitly building a\n",
    "model.\n",
    "\n",
    "\n",
    "# Other paradigms\n",
    "From here on, the goal changes.\n",
    "\n",
    "## Unsupervised learning\n",
    "In unsupervised learning, the goal is to glean insight from the data.\n",
    "\n",
    "### Clustering\n",
    "The goal of clustering is to group samples that are similar in some sense.\n",
    "The main difficulty is defining the notion of similarity.\n",
    "\n",
    "The most common algorithms are `k-means`, `hiearchical clustering`.\n",
    "\n",
    "### Dimensionality reduction\n",
    "The goal of dimensionality reduction is to summarize the data by reducing the\n",
    "number of variables. We can do that by\n",
    "\n",
    "- selecting a subset of important variables;\n",
    "- projecting onto another space (`PCA`, `t-SNE`, `feature learning`).\n",
    "\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement learning is a complex setting where the goal is for an agent to\n",
    "learn a policy describing how it should behave in a (possibly only partially\n",
    "observable) environment.\n",
    "\n",
    "\n",
    "## Density estimation\n",
    "\n",
    "> [TODO](https://en.wikipedia.org/wiki/Density_estimation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}