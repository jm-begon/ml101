{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "______                 _            _          _   _\n",
    "| ___ \\               | |          (_)        | | (_)\n",
    "| |_/ /___  __ _ _   _| | __ _ _ __ _ ______ _| |_ _  ___  _ __\n",
    "|    // _ \\/ _` | | | | |/ _` | '__| |_  / _` | __| |/ _ \\| '_ \\\n",
    "| |\\ \\  __/ (_| | |_| | | (_| | |  | |/ / (_| | |_| | (_) | | | |\n",
    "\\_| \\_\\___|\\__, |\\__,_|_|\\__,_|_|  |_/___\\__,_|\\__|_|\\___/|_| |_|\n",
    "            __/ |\n",
    "           |___/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Empirical risk minimization\n",
    "So far, all models were trained on the `empirical minimization principle` (ERM).\n",
    "That is, the model minimizing the error on the training set was selected. As we\n",
    "have seen, this may lead to overfitting when the hypothesis space is too\n",
    "complex.\n",
    "\n",
    "Mechanisms that try to force a suboptimal choice on the training set are called\n",
    "`regularizers`. They can work by\n",
    "- limiting the complexity of the hypothesis space;\n",
    "- explicitly changing the objective.\n",
    "\n",
    "# Regularization with decision trees\n",
    "Decision trees limit the complexity by not fully developing the decision tree,\n",
    "or pruning it at a later stage.\n",
    "\n",
    "There are several parameters which can dictate the shape of tree: limiting the\n",
    "maximum depth, imposing a minimum  number of samples in each leaf, or a minimum\n",
    "improvement, etc.\n",
    "\n",
    "Modern approaches tend to supplant decision trees with decision forests, which\n",
    "is a more subtle way of regularizing.\n",
    "\n",
    "# Regularization with linear methods\n",
    "With linear methods, the most common forms of regularization consist in changing\n",
    "the objective, typically by\n",
    "- imposing a $L2$ penalty on the weights ([ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html));\n",
    "- imposing a sparsity inducing penalty ([lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html));\n",
    "- or a combination of both ([elastic net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)).\n",
    "\n",
    "There are also more involved methods such as imposing group penalties and so on.\n",
    "How the penalization will affect the model is different in each set of\n",
    "circumstances.\n",
    "\n",
    "In all cases, there is usually a hyper-parameter controlling the importance of\n",
    "the penalization (somewhat akin to Lagrangian parameters in optimization).\n",
    "The effect and outcome of the penalization are tightly linked to this\n",
    "hyper-parameter!\n",
    "\n",
    "> TODO example with them ?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}