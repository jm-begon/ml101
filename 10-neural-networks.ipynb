{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    " _   _                      _              _                      _       _\n",
    "| \\ | |                    | |            | |                    | |     | |\n",
    "|  \\| | ___ _   _ _ __ __ _| |  _ __   ___| |___      _____  _ __| | ____| |___\n",
    "| . ` |/ _ \\ | | | '__/ _` | | | '_ \\ / _ \\ __\\ \\ /\\ / / _ \\| '__| |/ / _` / __|\n",
    "| |\\  |  __/ |_| | | | (_| | | | | | |  __/ |_ \\ V  V / (_) | |  |   < (_| \\__ \\\n",
    "\\_| \\_/\\___|\\__,_|_|  \\__,_|_| |_| |_|\\___|\\__| \\_/\\_/ \\___/|_|  |_|\\_\\__,_|___/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural network architectures and training\n",
    "Formally, a (feed-forward) neural network is composition of parametrized\n",
    "functions:\n",
    "> $$\\hat{y}(x ; \\Theta) = f_L(x, \\theta_L) \\circ \\ldots \\circ f_1(x, \\theta_1)$$\n",
    "> where $L$ is number of layers and $\\Theta = [\\theta_1, \\ldots, \\theta_L]$ is\n",
    "> the set of trainable parameters.\n",
    "\n",
    "The architecture of the network is dictated by the nature of the layer\n",
    "functions. The most common architectures are fully-connected feed-forward\n",
    "rectified networks, which is to say that $f_l$ is a linear mapping for $l$ odds\n",
    "and $f_l$ are `ReLU` functions (applied point-wise) for $l$ even.\n",
    "\n",
    "A ReLU is the function\n",
    "> $$ReLU(x) = \\begin{cases}\n",
    ">   x, & \\text{ if } x \\geq 0 \\\\\n",
    ">   0, & \\text{ otherwise} \\end{cases}$$\n",
    "\n",
    "## Classification\n",
    "For classification a softmax layer is appended at the end of the network,\n",
    "at least during training. Typically, the cross-entropy loss is used with one-hot\n",
    "encoding.\n",
    "\n",
    "A softmax function is\n",
    "> $$\\text{softmax}(x)^{(j)} = \\frac{e^{x^{(j)}}}{\\sum_{k=1}^K e^{x^{(k)}}}$$\n",
    "> where $x^{(j)}$ indicates the $j$th component of the vector $x$.\n",
    "\n",
    "## Backpropagation\n",
    "A neural network is trained by mini-batch gradient descent (but often\n",
    "referred to as `stochastic gradient descent`, SGD). A each step, a\n",
    "subset of training instances are run through the network, gradient of the loss\n",
    "is computed, and the weights are updated in the direction which reduces the\n",
    "gradient the most.\n",
    "\n",
    "Owing to the layered nature of the network, the gradient of earlier layers\n",
    "must be computed thanks to the chain rule of differentiation. This is known as\n",
    "the `backpropagation` algorithm\n",
    "\n",
    "# Fully-connected feed-forward networks\n",
    "\n",
    "> TODO examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}